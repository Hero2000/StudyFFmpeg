# 解码视频流，使用 UIImageView 渲染

> 视频的渲染要比音频渲染简单，因此为了激发学习兴趣，先从视频说起！

在第二天，我们学习了如何找到视频流，并且获取视频相关的信息，比如: 解码器id，视频像素格式，帧率，宽高等重要信息。接下来继续往下学习，拿到这个信息之后，我们要从流里拿到包(制作视频时经过编码器压缩之后的)，然后解码成帧并通过一定的技术渲染出来。

## 基本流程

读包和解码作为生产者，是一个很耗时的操作，所以一定要在子线程里面完成，将可渲染的 image 对象放到缓存队列中。

渲染循环的存在则是为了检测队列是否有可以播放的画面，没有就定时轮训检测，有的话就根据上一帧画面的时长定时，时间到了去播放下一帧。

### 读包解码循环

```
打开文件 -> 去取视频流 -> 确定解码器，视频宽高等信息 ->
        
|-----------<--------|
∨                    |
-> 读包               |
-> 解码(YUV)          |
-> 转换(NV12)         |
-> CVPixelBufferRef  |[循环]
-> CIImage           |
-> CGImageRef        |
-> UIImage           |
-> 内存队列           ∧
-> --------->--------|
```

### 渲染驱动循环

```
通过定时器定期从内存队列拿数据，由于这种渲染
方式太卡，所以只有缓存 3s 可播数据，才开始播放

[
videoFrame,
videoFrame,
.           -->--[缓存队列]--
.                          |
.                          |
]                          |[循环]
                           |
                           |
/#/#/#/#/                  |
/#/#/#/#/ --<---[屏幕]------
/#/#/#/#/
```

**注：**

实际生产环境根本不会使用这种方式渲染，因为性能太差了，其主要瓶颈在于 CIImage 转成 UIImage 太耗时，我使用模拟器测试的，转换一次平均需要 0.3s ！另外读包和解码也应该放在不同的线程里进行，防止因为解码耗时导致读包阻塞；后续版本会解决这些问题。

## 核心代码

在 io_queue 中读包解码:

```
- (void)startReadFrames
{
    if (!self.io_queue) {
        dispatch_queue_t io_queue = dispatch_queue_create("read-io", DISPATCH_QUEUE_SERIAL);
        self.io_queue = io_queue;
    }
    
    __weakSelf__
    dispatch_async(self.io_queue, ^{
        
        while (1) {
            AVPacket pkt;
            __strongSelf__
            // 读包
            if (av_read_frame(self->_formatCtx,&pkt) >= 0) {
                
                // 只处理视频
                if (pkt.stream_index == self.stream_index_video) {
                    
                    __weakSelf__
                    // 解码
                    [self handleVideoPacket:&pkt completion:^(AVFrame *video_frame) {
                        __strongSelf__
                        
                        int pictRet = sws_scale(self.img_convert_ctx, (const uint8_t* const*)video_frame->data, video_frame->linesize, 0, self.vheight, self.pFrameYUV->data, self.pFrameYUV->linesize);
                        if (pictRet <= 0) {
                            return ;
                        }
                        UIImage *img = [self imageFromAVFrame:self.pFrameYUV width:self.vwidth height:self.vheight];
                        
                        if (img) {
                            // 获取时长
                            const double frameDuration = av_frame_get_pkt_duration(video_frame) * self.videoTimeBase;
                            
                            // 构造模型
                            MRVideoFrame *frame = [[MRVideoFrame alloc]init];
                            frame.duration = frameDuration;
                            frame.image = img;
                            // 存放到内存
                            @synchronized(self) {
                                [self.videoFrames addObject:frame];
                                if (!self.bufferOk) {
                                    self.bufferOk = [self checkIsBufferOK];
                                }
                            }
                        }
                    }];
                }
            }else{
                NSLog(@"eof,stop read more frame!");
                MRVideoFrame *frame = [[MRVideoFrame alloc]init];
                frame.eof = YES;
                @synchronized(self) {
                    [self.videoFrames addObject:frame];
                    if (!self.bufferOk) {
                        self.bufferOk = [self checkIsBufferOK];
                    }
                }
                break;
            }
            ///释放内存
            av_packet_unref(&pkt);
        }
    });
}
```

avframe YUV 数据转成 nv12 格式

```
///初始化
enum AVPixelFormat pix_fmt = PIX_FMT_NV12;
#endif
const int picSize = avpicture_get_size(pix_fmt, self.vwidth, self.vheight);
    
self.out_buffer = malloc(picSize);
self.img_convert_ctx = sws_getContext(self.vwidth, self.vheight, self.format, self.vwidth, self.vheight, pix_fmt, SWS_BICUBIC, NULL, NULL, NULL);
    
self.pFrameYUV = av_frame_alloc();
avpicture_fill((AVPicture *)self.pFrameYUV, self.out_buffer, pix_fmt, self.vwidth, self.vheight);

/// 转化
int pictRet = sws_scale(self.img_convert_ctx, (const uint8_t* const*)video_frame->data, video_frame->linesize, 0, self.vheight, self.pFrameYUV->data, self.pFrameYUV->linesize);

y: self.pFrameYUV->data[0]
uv: self.pFrameYUV->data[1]

```

NV12数据转成 UIImage

```
#pragma mark - YUV(NV12)-->CIImage--->UIImage
//https://stackoverflow.com/questions/25659671/how-to-convert-from-yuv-to-ciimage-for-ios
- (UIImage *)imageFromAVFrame:(AVFrame *)pFrame width:(int)width height:(int)height
{
    //YUV(NV12)-->CIImage--->UIImage Conversion
    NSDictionary *pixelAttributes = @{(NSString*)kCVPixelBufferIOSurfacePropertiesKey:@{}};
    
    CVPixelBufferRef pixelBuffer = NULL;
    
    CVReturn result = CVPixelBufferCreate(kCFAllocatorDefault,
                                          w,
                                          h,
                                          kCVPixelFormatType_420YpCbCr8BiPlanarFullRange,
                                          (__bridge CFDictionaryRef)(pixelAttributes),
                                          &pixelBuffer);
    
    CVPixelBufferLockBaseAddress(pixelBuffer,0);
    unsigned char *yDestPlane = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 0);
    
    // Here y_ch0 is Y-Plane of YUV(NV12) data.
    
    unsigned char *y_ch0 = pFrame->data[0];
    unsigned char *y_ch1 = pFrame->data[1];
    
    memcpy(yDestPlane, y_ch0, w * h);
    unsigned char *uvDestPlane = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 1);
    
    // Here y_ch1 is UV-Plane of YUV(NV12) data.
    memcpy(uvDestPlane, y_ch1, w * h / 2.0);
    CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);
    
    if (result != kCVReturnSuccess) {
        NSLog(@"Unable to create cvpixelbuffer %d", result);
    }
    
    UIImage *image = [self cvPixelBufferReftoUIImage:pixelBuffer w:w h:h];
    return image;
}
```

CVPixelBuffer 转成 UIImage

```
/性能是很差,平均需要 0.3s，很不适合用来渲染视频 ！！！
- (UIImage *)cvPixelBufferReftoUIImage:(CVPixelBufferRef)pixelBuffer w:(int)w h:(int)h
{
    // CIImage Conversion
    CIImage *coreImage = [CIImage imageWithCVPixelBuffer:pixelBuffer];
    CIContext *context = [CIContext contextWithOptions:nil];
    ///引发内存泄露? https://stackoverflow.com/questions/32520082/why-is-cicontext-createcgimage-causing-a-memory-leak
    NSTimeInterval begin = CFAbsoluteTimeGetCurrent();
    CGImageRef cgImage = [context createCGImage:coreImage
                                       fromRect:CGRectMake(0, 0, w, h)];
    NSTimeInterval end = CFAbsoluteTimeGetCurrent();
    // UIImage Conversion
    UIImage *uiImage = [[UIImage alloc] initWithCGImage:cgImage
                                                  scale:1.0
                                            orientation:UIImageOrientationUp];
    
    NSLog(@"decode an image cost :%g",end-begin);
    CVPixelBufferRelease(pixelBuffer);
    CGImageRelease(cgImage);
    return uiImage;
}
```

## 改进

**将 USEBITMAP 宏的值改为 1，逻辑将会变化，YUV将会转化成 RGB24 ！**

然后使用 iOS 位图创建图片:

```
- (UIImage *)imageFromAVFrame:(AVFrame*)video_frame width:(int)width height:(int)height
{
    const UInt8 *rgb = video_frame->data[0];
    size_t bytesPerRow = video_frame->linesize[0];
    CFIndex length = bytesPerRow*height;
    
    CGBitmapInfo bitmapInfo = kCGBitmapByteOrderDefault;
    ///需要copy！因为video_frame是
    CFDataRef data = CFDataCreate(kCFAllocatorDefault, rgb, length);
    CGDataProviderRef provider = CGDataProviderCreateWithCFData(data);
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    
    CGImageRef cgImage = CGImageCreate(width,
                                       height,
                                       8,
                                       24,
                                       bytesPerRow,
                                       colorSpace,
                                       bitmapInfo,
                                       provider,
                                       NULL,
                                       NO,
                                       kCGRenderingIntentDefault);
    CGColorSpaceRelease(colorSpace);
    
    UIImage *image = [UIImage imageWithCGImage:cgImage];
    CGImageRelease(cgImage);
    CGDataProviderRelease(provider);
    CFRelease(data);
    
    return image;
}
```

这个效率高出很多，基本可以正常观看，画面略卡，但这样一来读包解码线程压力小了，导致疯狂读包解码，内存急速飙升！这问题以后解决，我们先把重点放在学习视频画面渲染上！