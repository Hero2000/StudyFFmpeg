# 解码视频流，使用 UIImageView 渲染

> 视频的渲染要比音频渲染简单，因此为了激发学习兴趣，先从视频说起！

在第二天，我们学习了如何找到视频流，并且获取视频相关的信息，比如: 解码器id，视频像素格式，帧率，宽高等重要信息。接下来继续往下学习，拿到这个信息之后，我们要从流里拿到包(制作视频时经过编码器压缩之后的)，然后解码成帧并通过一定的技术渲染出来。

## 基本流程

读包和解码作为生产者，是一个很耗时的操作，所以一定要在子线程里面完成，将可渲染的 image 对象放到缓存队列中。

渲染循环的存在则是为了检测队列是否有可以播放的画面，没有就定时轮训检测，有的话就根据上一帧画面的时长定时，时间到了去播放下一帧。

### 读包解码循环

```
打开文件 -> 去取视频流 -> 确定解码器，视频宽高等信息 ->
        
|-----------<--------|
∨                    |
-> 读包               |
-> 解码(YUV)          |
-> CVPixelBufferRef  |[循环]
-> CIImage           |
-> CGImageRef        |
-> UIImage           |
-> 内存队列           ∧
-> --------->--------|
```

### 渲染驱动循环

```
通过定时器定期从内存队列拿数据，由于这种渲染
方式太卡，所以只有缓存 3s 可播数据，才开始播放

[
videoFrame,
videoFrame,
.           -->--[缓存队列]--
.                          |
.                          |
]                          |[循环]
                           |
                           |
/#/#/#/#/                  |
/#/#/#/#/ --<---[屏幕]------
/#/#/#/#/
```

**注：**

实际生产环境根本不会使用这种方式渲染，因为性能太差了，其主要瓶颈在于 CIImage 转成 UIImage 太耗时，我使用模拟器测试的，转换一次平均需要 0.3s ！另外读包和解码也应该放在不同的线程里进行，防止因为解码耗时导致读包阻塞；后续版本会解决这些问题。

## 核心代码

在 io_queue 中读包解码:

```
- (void)startReadFrames
{
    if (!self.io_queue) {
        dispatch_queue_t io_queue = dispatch_queue_create("read-io", DISPATCH_QUEUE_SERIAL);
        self.io_queue = io_queue;
    }
    
    __weakSelf__
    dispatch_async(self.io_queue, ^{
        
        while (1) {
            
            AVPacket pkt;
            __strongSelf__
            // 读包
            if (av_read_frame(_formatCtx,&pkt) >= 0) {
                
                ///处理视频流
                if (pkt.stream_index == self.stream_index_video) {
                    ///解码
                    __weakSelf__
                    [self handleVideoPacket:&pkt completion:^(AVFrame *video_frame) {
                        __strongSelf__
                        ///目前只处理了 YUV420P 和 YUVJ420P 格式
                        if (video_frame->format == AV_PIX_FMT_YUV420P || video_frame->format == AV_PIX_FMT_YUVJ420P) {
                            unsigned char *nv12 = NULL;
                            
                            int nv12Size = AVFrameConvertToNV12Buffer(video_frame,&nv12);
                            if (nv12Size > 0){
                                NSTimeInterval begin = CFAbsoluteTimeGetCurrent();
                                
                                //0.000494003
                                CVPixelBufferRef pixelBuffer = [self NV12toCVPixelBufferRef:self.width h:self.height linesize:video_frame->linesize[0] buffer:nv12 size:nv12Size];
                                CIImage *ciImage = [CIImage imageWithCVPixelBuffer:pixelBuffer];
                                
                                NSTimeInterval end = CFAbsoluteTimeGetCurrent();
                                NSLog(@"decode an image cost :%g",end-begin);
                                free(nv12);
                                nv12 = NULL;
                                const double frameDuration = av_frame_get_pkt_duration(video_frame) * self.videoTimeBase;
                                MRVideoFrame *frame = [[MRVideoFrame alloc]init];
                                frame.duration = frameDuration;
                                frame.ciImage = ciImage;
                                
                                @synchronized(self) {
                                    [self.videoFrames addObject:frame];
                                    if (!self.bufferOk) {
                                        self.bufferOk = [self checkIsBufferOK];
                                    }
                                }
                            }
                        }
                    }];
                }
            }else{
                NSLog(@"eof,stop read more frame!");
                MRVideoFrame *frame = [[MRVideoFrame alloc]init];
                frame.eof = YES;
                @synchronized(self) {
                    [self.videoFrames addObject:frame];
                    if (!self.bufferOk) {
                        self.bufferOk = [self checkIsBufferOK];
                    }
                }
                break;
            }
            ///释放内存
            av_packet_unref(&pkt);
        }
    });
}
```

avframe YUV 数据转成 nv12 格式

```
int AVFrameConvertToNV12Buffer(AVFrame *pFrame,unsigned char **nv12)
{
    if ((pFrame->format == AV_PIX_FMT_YUV420P) || (pFrame->format == AV_PIX_FMT_NV12) || (pFrame->format == AV_PIX_FMT_NV21) || (pFrame->format == AV_PIX_FMT_YUVJ420P)){
        
        int height = pFrame->height;
        int width = pFrame->width;
        //计算需要分配的内存大小
        int needSize = height * width * 1.5;
        
        if (*nv12 == NULL) {
            //申请内存
            *nv12 = malloc(needSize);
        }
        unsigned char *buf = *nv12;
        unsigned char *y = pFrame->data[0];
        unsigned char *u = pFrame->data[1];
        unsigned char *v = pFrame->data[2];
        
        unsigned int ys = pFrame->linesize[0];
        
        //先写入Y（height * width 个 Y 数据）
        int offset=0;
        for (int i=0; i < height; i++)
        {
            memcpy(buf+offset,y + i * ys, width);
            offset+=width;
        }
        
        ///一个U一个V交替着排列（一共有 width * height / 4 个 [U + V] ）
        for (int i = 0; i < width * height / 4; i++)
        {
            memcpy(buf+offset,u + i, 1);
            offset++;
            memcpy(buf+offset,v + i, 1);
            offset++;
        }
        return needSize;
    }
    
    return -1;
}
```

NV12 转成 CVPixelBuffer

```
#pragma mark - YUV(NV12)-->CIImage--->UIImage
//https://stackoverflow.com/questions/25659671/how-to-convert-from-yuv-to-ciimage-for-ios
-(UIImage *)NV12toUIImage:(int)w h:(int)h buffer:(unsigned char *)buffer
{
    //YUV(NV12)-->CIImage--->UIImage Conversion
    NSDictionary *pixelAttributes = @{(NSString*)kCVPixelBufferIOSurfacePropertiesKey:@{}};
    
    CVPixelBufferRef pixelBuffer = NULL;
    
    CVReturn result = CVPixelBufferCreate(kCFAllocatorDefault,
                                          w,
                                          h,
                                          kCVPixelFormatType_420YpCbCr8BiPlanarFullRange,
                                          (__bridge CFDictionaryRef)(pixelAttributes),
                                          &pixelBuffer);
    
    CVPixelBufferLockBaseAddress(pixelBuffer,0);
    unsigned char *yDestPlane = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 0);
    
    // Here y_ch0 is Y-Plane of YUV(NV12) data.
    unsigned char *y_ch0 = buffer;
    unsigned char *y_ch1 = buffer + w * h;
    memcpy(yDestPlane, y_ch0, w * h);
    unsigned char *uvDestPlane = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 1);
    
    // Here y_ch1 is UV-Plane of YUV(NV12) data.
    memcpy(uvDestPlane, y_ch1, w * h/2);
    CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);
    
    if (result != kCVReturnSuccess) {
        NSLog(@"Unable to create cvpixelbuffer %d", result);
    }
    
    UIImage *image = [self cvPixelBufferReftoUIImage:pixelBuffer w:w h:h];
    return image;
}
```

CVPixelBuffer 转成 UIImage

```
//性能是很差,平均需要 0.3s，很不适合用来渲染视频 ！！！
- (UIImage *)cvPixelBufferReftoUIImage:(CVPixelBufferRef)pixelBuffer w:(int)w h:(int)h
{
    // CIImage Conversion
    CIImage *coreImage = [CIImage imageWithCVPixelBuffer:pixelBuffer];
    
    NSTimeInterval begin = CFAbsoluteTimeGetCurrent();
    CIContext *context = [CIContext contextWithOptions:nil];
    
    ///引发内存泄露? https://stackoverflow.com/questions/32520082/why-is-cicontext-createcgimage-causing-a-memory-leak
    CGImageRef cgImage = [context createCGImage:coreImage
                                       fromRect:CGRectMake(0, 0, w, h)];
    
    // UIImage Conversion
    UIImage *uiImage = [[UIImage alloc] initWithCGImage:cgImage
                                                  scale:1.0
                                            orientation:UIImageOrientationUp];
    
    CVPixelBufferRelease(pixelBuffer);
    CGImageRelease(cgImage);
    NSTimeInterval end = CFAbsoluteTimeGetCurrent();
    NSLog(@"decode an image cost :%g",end-begin);
    return uiImage;
}
```